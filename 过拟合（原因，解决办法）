最终表现：
在训练集上表现好；在测试集上效果差；泛化能力弱
原因：
1. 数据样本不够/抽取的样本不具代表性
2. 模型过于复杂，参数过多
3. 样本中噪声干扰过大

解决办法：
1.针对数据样本不够：
  ❤️（1）采用数据增强
2. 建立合适的模型： 
  ❤️（1）正则化：
      在训练时向loss function中添加惩罚项，用来限制权值大小，降低模型复杂度；常采用L1/L2正则化；L1使用绝对值最小化权值，会将一些不相关的特征强制到0，产生特征的稀疏矩阵，有特征选择的特点；L2使用平方和最小化权值，有相对较小的权值，得到的结果比较平滑
  （2）early stopping
3.结合多种模型：训练多个模型，取各个模型结果的平均
  ❤️（1）dropout（也是针对深度模型的解决方法）：每输入一个mini-batch，只开启的部分的神经元，使是输入前向传播，并在反向传播后只更新这一部分神经元权重；以此重复。这种做法相当于降低了模型复杂度，减小每次训练的参数；也减少了神经元间的共适应性，因为无法保证两个神经元每次都同时出现，这样权值的更新不依赖于神经元之间的隐性关联；也相当于使用了不同模型之后ensemble，提高模型泛化能力；
  （2）bagging
  （3）boosting
4. batchnormalization
